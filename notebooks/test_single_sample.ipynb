{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöó BEV Segmentation Test\n",
        "\n",
        "Load trained model ‚Üí Run inference ‚Üí Compare with ground truth ‚Üí Visualize results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports successful\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from src.model import BEVSegmentationModel\n",
        "from src.training import MetricsCalculator\n",
        "from src.visualization import create_default_visualizer\n",
        "\n",
        "print(\"‚úÖ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing BEV Segmentation Model...\n",
            "  - Config loaded from: ../runs/run_20250915_041348/config.yaml\n",
            "  - Backbone: resnet50\n",
            "  - Pretrained: True\n",
            "  - Freeze backbone: False\n",
            "  - Number of classes: 7\n",
            "  - BEV size: 256x256\n",
            "Initializing resnet50 backbone...\n",
            "  - Pretrained: True\n",
            "  - Freeze backbone: False\n",
            "  - Feature dimension: 2048 -> 64\n",
            "  - Output stride: 32\n",
            "Initializing BEV Encoder...\n",
            "  - Input channels: 64\n",
            "  - Hidden channels: 128\n",
            "  - Output channels: 128\n",
            "  - Encoder architecture: 64 -> 128 -> 128\n",
            "  - Downsampling: 256x256 -> 128x128 -> 64x64\n",
            "Initializing Decoder...\n",
            "  - Input channels: 128\n",
            "  - Number of classes: 7\n",
            "  - Target BEV size: 256x256\n",
            "  - Decoder architecture: 128 -> 128 -> 64 -> 7\n",
            "  - Upsampling: 64x64 -> 128x128 -> 256x256\n",
            "  - Temporal fusion disabled\n",
            "  - All components initialized successfully\n",
            "üìÅ Loading trained model from: ../runs/run_20250915_041348/checkpoints/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aftab\\AppData\\Local\\Temp\\ipykernel_21024\\255893444.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_pattern, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Trained model loaded!\n",
            "üìä Training epoch: 4\n",
            "üìä Best validation mIoU: unknown\n",
            "üìä Training loss: unknown\n",
            "üìä Model parameters: 24,752,071\n"
          ]
        }
      ],
      "source": [
        "# --- Load Configuration & Trained Model ---\n",
        "import glob\n",
        "\n",
        "# Load config from the specific run\n",
        "config_path = \"../runs/run_20250915_041348/config.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "device = torch.device(config['device'])\n",
        "\n",
        "# Initialize model architecture\n",
        "model = BEVSegmentationModel(config_path=config_path).to(device)\n",
        "\n",
        "# Load trained weights\n",
        "checkpoint_pattern = \"../runs/run_20250915_041348/checkpoints/best_model.pth\"\n",
        "if os.path.exists(checkpoint_pattern):\n",
        "    print(f\"üìÅ Loading trained model from: {checkpoint_pattern}\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_pattern, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"‚úÖ Trained model loaded!\")\n",
        "    print(f\"üìä Training epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
        "    \n",
        "    # Handle metrics safely\n",
        "    best_miou = checkpoint.get('best_val_miou', None)\n",
        "    train_loss = checkpoint.get('train_loss', None)\n",
        "    \n",
        "    if best_miou is not None:\n",
        "        print(f\"üìä Best validation mIoU: {best_miou:.4f}\")\n",
        "    else:\n",
        "        print(f\"üìä Best validation mIoU: unknown\")\n",
        "        \n",
        "    if train_loss is not None:\n",
        "        print(f\"üìä Training loss: {train_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"üìä Training loss: unknown\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No trained model found!\")\n",
        "    print(\"Available checkpoints:\")\n",
        "    checkpoints = glob.glob(\"../runs/run_20250915_041348/checkpoints/*.pth\")\n",
        "    for ckpt in checkpoints:\n",
        "        print(f\"  - {os.path.basename(ckpt)}\")\n",
        "    raise FileNotFoundError(\"No trained model checkpoint found!\")\n",
        "\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Found 3731 validation samples\n",
            "üé≤ Testing 5 random samples:\n",
            "  1. v_2_0053000\n",
            "  2. v_1_0089000\n",
            "  3. v_3_0144500\n",
            "  4. v_4_0039500\n",
            "  5. v_3_0155500\n",
            "\n",
            "üì∏ Loading sample: v_2_0053000\n",
            "‚úÖ Sample loaded:\n",
            "  - front: torch.Size([3, 256, 256])\n",
            "  - left: torch.Size([3, 256, 256])\n",
            "  - right: torch.Size([3, 256, 256])\n",
            "  - rear: torch.Size([3, 256, 256])\n",
            "  - BEV GT: torch.Size([256, 256])\n"
          ]
        }
      ],
      "source": [
        "# --- Load Random Examples from Validation Set ---\n",
        "import random\n",
        "import glob\n",
        "\n",
        "def load_image(path, target_size=(256, 256)):\n",
        "    \"\"\"Load and preprocess image\"\"\"\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img_array = np.array(img) / 255.0\n",
        "    \n",
        "    # Remove white fade effect by normalizing to full range\n",
        "    if img_array.max() > img_array.min():\n",
        "        img_array = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n",
        "    \n",
        "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float()\n",
        "    return img_tensor\n",
        "\n",
        "def load_bev_gt(path, target_size=(256, 256)):\n",
        "    \"\"\"Load and preprocess BEV ground truth\"\"\"\n",
        "    img = Image.open(path).convert('L')\n",
        "    img = img.resize(target_size)\n",
        "    img_array = np.array(img)\n",
        "    img_tensor = torch.from_numpy(img_array).long()\n",
        "    return img_tensor\n",
        "\n",
        "# Find all validation samples\n",
        "val_data_root = \"../data/cam2bev-data-master-1_FRLR/1_FRLR/val\"\n",
        "front_images = glob.glob(f\"{val_data_root}/front/front/*.png\")\n",
        "print(f\"üìÇ Found {len(front_images)} validation samples\")\n",
        "\n",
        "# Pick random samples to test\n",
        "num_samples = 5  # Test 5 random samples\n",
        "random_samples = random.sample(front_images, min(num_samples, len(front_images)))\n",
        "\n",
        "print(f\"üé≤ Testing {len(random_samples)} random samples:\")\n",
        "for i, sample_path in enumerate(random_samples):\n",
        "    sample_id = os.path.basename(sample_path).replace('.png', '')\n",
        "    print(f\"  {i+1}. {sample_id}\")\n",
        "\n",
        "# Load first random sample\n",
        "sample_path = random_samples[0]\n",
        "sample_id = os.path.basename(sample_path).replace('.png', '')\n",
        "\n",
        "print(f\"\\nüì∏ Loading sample: {sample_id}\")\n",
        "\n",
        "# Load all camera views for this sample\n",
        "camera_images = {}\n",
        "for view in ['front', 'left', 'right', 'rear']:\n",
        "    view_path = sample_path.replace('/front/front/', f'/{view}/{view}/')\n",
        "    if os.path.exists(view_path):\n",
        "        camera_images[view] = load_image(view_path, (config['img_height'], config['img_width']))\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Missing {view} camera for {sample_id}\")\n",
        "\n",
        "# Load BEV ground truth\n",
        "bev_path = sample_path.replace('/front/front/', '/bev/bev/')\n",
        "if os.path.exists(bev_path):\n",
        "    bev_gt_tensor = load_bev_gt(bev_path, (config['bev_height'], config['bev_width']))\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Missing BEV ground truth for {sample_id}\")\n",
        "\n",
        "print(\"‚úÖ Sample loaded:\")\n",
        "for view, img in camera_images.items():\n",
        "    print(f\"  - {view}: {img.shape}\")\n",
        "print(f\"  - BEV GT: {bev_gt_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Testing multiple random samples...\n",
            "\n",
            "--- Sample 1/5: v_2_0053000 ---\n",
            "  Accuracy: 0.0000\n",
            "  mIoU: 0.0000\n",
            "  Classes: 6\n",
            "\n",
            "--- Sample 2/5: v_1_0089000 ---\n",
            "  Accuracy: 0.0012\n",
            "  mIoU: 0.0097\n",
            "  Classes: 7\n",
            "\n",
            "--- Sample 3/5: v_3_0144500 ---\n",
            "  Accuracy: 0.0000\n",
            "  mIoU: 0.0000\n",
            "  Classes: 0\n",
            "\n",
            "--- Sample 4/5: v_4_0039500 ---\n",
            "  Accuracy: 0.0000\n",
            "  mIoU: 0.0000\n",
            "  Classes: 2\n",
            "\n",
            "--- Sample 5/5: v_3_0155500 ---\n",
            "  Accuracy: 0.0000\n",
            "  mIoU: 0.0000\n",
            "  Classes: 2\n",
            "\n",
            "üìä Summary across 5 samples:\n",
            "  Average Accuracy: 0.0003\n",
            "  Average mIoU: 0.0019\n",
            "  Best mIoU: v_1_0089000 (0.0097)\n",
            "  Worst mIoU: v_3_0144500 (0.0000)\n"
          ]
        }
      ],
      "source": [
        "# --- Test Multiple Random Samples ---\n",
        "# Initialize metrics calculator\n",
        "class_names = {0: \"unlabeled\", 1: \"car\", 2: \"vegetation\", 3: \"road\", 4: \"terrain\", 5: \"guard_rail\", 6: \"sidewalk\"}\n",
        "metrics_calc = MetricsCalculator(config['num_classes'], class_names)\n",
        "\n",
        "print(\"üî¨ Testing multiple random samples...\")\n",
        "\n",
        "all_results = []\n",
        "for i, sample_path in enumerate(random_samples):\n",
        "    sample_id = os.path.basename(sample_path).replace('.png', '')\n",
        "    print(f\"\\n--- Sample {i+1}/{len(random_samples)}: {sample_id} ---\")\n",
        "    \n",
        "    # Load camera images for this sample\n",
        "    camera_images = {}\n",
        "    for view in ['front', 'left', 'right', 'rear']:\n",
        "        view_path = sample_path.replace('/front/front/', f'/{view}/{view}/')\n",
        "        if os.path.exists(view_path):\n",
        "            camera_images[view] = load_image(view_path, (config['img_height'], config['img_width']))\n",
        "    \n",
        "    # Load BEV ground truth\n",
        "    bev_path = sample_path.replace('/front/front/', '/bev/bev/')\n",
        "    if not os.path.exists(bev_path):\n",
        "        print(f\"‚ö†Ô∏è  Skipping {sample_id} - missing BEV ground truth\")\n",
        "        continue\n",
        "        \n",
        "    bev_gt_tensor = load_bev_gt(bev_path, (config['bev_height'], config['bev_width']))\n",
        "    \n",
        "    # Run inference\n",
        "    camera_inputs = {view: img.unsqueeze(0).to(device) for view, img in camera_images.items()}\n",
        "    bev_gt_input = bev_gt_tensor.unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions = model(camera_inputs)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = metrics_calc.calculate_metrics(predictions, bev_gt_input)\n",
        "    \n",
        "    # Store results\n",
        "    result = {\n",
        "        'sample_id': sample_id,\n",
        "        'accuracy': metrics['accuracy'],\n",
        "        'miou': metrics['mean_iou'],\n",
        "        'present_classes': metrics['present_classes']\n",
        "    }\n",
        "    all_results.append(result)\n",
        "    \n",
        "    print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"  mIoU: {metrics['mean_iou']:.4f}\")\n",
        "    print(f\"  Classes: {metrics['present_classes']}\")\n",
        "\n",
        "# Summary statistics\n",
        "if all_results:\n",
        "    avg_accuracy = np.mean([r['accuracy'] for r in all_results])\n",
        "    avg_miou = np.mean([r['miou'] for r in all_results])\n",
        "    \n",
        "    print(f\"\\nüìä Summary across {len(all_results)} samples:\")\n",
        "    print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
        "    print(f\"  Average mIoU: {avg_miou:.4f}\")\n",
        "    \n",
        "    # Show best and worst samples\n",
        "    best_sample = max(all_results, key=lambda x: x['miou'])\n",
        "    worst_sample = min(all_results, key=lambda x: x['miou'])\n",
        "    \n",
        "    print(f\"  Best mIoU: {best_sample['sample_id']} ({best_sample['miou']:.4f})\")\n",
        "    print(f\"  Worst mIoU: {worst_sample['sample_id']} ({worst_sample['miou']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñºÔ∏è  Creating visualizations for all 5 samples...\n",
            "Initializing BEV Visualizer...\n",
            "  - Number of classes: 7\n",
            "  - Class names: ['unlabeled', 'car', 'vegetation', 'road', 'terrain', 'guard_rail', 'sidewalk']\n",
            "  Visualizing sample 1/5: v_2_0053000 (mIoU: 0.0000)\n",
            "    ‚úÖ Saved as './sample_v_2_0053000.png'\n",
            "  Visualizing sample 2/5: v_1_0089000 (mIoU: 0.0097)\n",
            "    ‚úÖ Saved as './sample_v_1_0089000.png'\n",
            "  Visualizing sample 3/5: v_3_0144500 (mIoU: 0.0000)\n",
            "    ‚úÖ Saved as './sample_v_3_0144500.png'\n",
            "  Visualizing sample 4/5: v_4_0039500 (mIoU: 0.0000)\n",
            "    ‚úÖ Saved as './sample_v_4_0039500.png'\n",
            "  Visualizing sample 5/5: v_3_0155500 (mIoU: 0.0000)\n",
            "    ‚úÖ Saved as './sample_v_3_0155500.png'\n",
            "\n",
            "‚úÖ All 5 samples visualized and saved!\n",
            "\n",
            "üèÜ Best sample: v_1_0089000 (mIoU: 0.0097)\n",
            "üìâ Worst sample: v_3_0144500 (mIoU: 0.0000)\n"
          ]
        }
      ],
      "source": [
        "# --- Visualize All Samples ---\n",
        "if all_results:\n",
        "    print(f\"üñºÔ∏è  Creating visualizations for all {len(all_results)} samples...\")\n",
        "    \n",
        "    # Create visualizer\n",
        "    visualizer = create_default_visualizer()\n",
        "    \n",
        "    for i, result in enumerate(all_results):\n",
        "        sample_id = result['sample_id']\n",
        "        print(f\"  Visualizing sample {i+1}/{len(all_results)}: {sample_id} (mIoU: {result['miou']:.4f})\")\n",
        "        \n",
        "        # Load camera images and BEV ground truth for this sample\n",
        "        camera_images_viz = {}\n",
        "        for view in ['front', 'left', 'right', 'rear']:\n",
        "            view_path = f\"../data/cam2bev-data-master-1_FRLR/1_FRLR/val/{view}/{view}/{sample_id}.png\"\n",
        "            if os.path.exists(view_path):\n",
        "                camera_images_viz[view] = load_image(view_path, (config['img_height'], config['img_width']))\n",
        "            else:\n",
        "                print(f\"    ‚ö†Ô∏è  Missing {view} camera for {sample_id}\")\n",
        "        \n",
        "        # Load BEV ground truth for this sample\n",
        "        bev_path = f\"../data/cam2bev-data-master-1_FRLR/1_FRLR/val/bev/bev/{sample_id}.png\"\n",
        "        if os.path.exists(bev_path):\n",
        "            bev_gt_viz = load_bev_gt(bev_path, (config['bev_height'], config['bev_width']))\n",
        "        else:\n",
        "            print(f\"    ‚ö†Ô∏è  Missing BEV ground truth for {sample_id}\")\n",
        "            continue\n",
        "        \n",
        "        # Run inference on this sample\n",
        "        camera_inputs_viz = {view: img.unsqueeze(0).to(device) for view, img in camera_images_viz.items()}\n",
        "        bev_gt_input_viz = bev_gt_viz.unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions_viz = model(camera_inputs_viz)\n",
        "        \n",
        "        # Create visualization\n",
        "        save_path = f\"./sample_{sample_id}.png\"\n",
        "        \n",
        "        visualizer.visualize_sample(\n",
        "            camera_images=camera_inputs_viz,\n",
        "            bev_features=None,\n",
        "            segmentation_logits=predictions_viz,\n",
        "            ground_truth=bev_gt_input_viz,\n",
        "            sample_id=sample_id,\n",
        "            save_path=save_path\n",
        "        )\n",
        "        \n",
        "        print(f\"    ‚úÖ Saved as '{save_path}'\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ All {len(all_results)} samples visualized and saved!\")\n",
        "    \n",
        "    # Show best and worst samples\n",
        "    best_sample = max(all_results, key=lambda x: x['miou'])\n",
        "    worst_sample = min(all_results, key=lambda x: x['miou'])\n",
        "    \n",
        "    print(f\"\\nüèÜ Best sample: {best_sample['sample_id']} (mIoU: {best_sample['miou']:.4f})\")\n",
        "    print(f\"üìâ Worst sample: {worst_sample['sample_id']} (mIoU: {worst_sample['miou']:.4f})\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to visualize\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
