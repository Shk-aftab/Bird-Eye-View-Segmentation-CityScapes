# üß† Source Code Documentation

This directory contains the core implementation of the BEV Segmentation Pipeline. Each module is designed to be modular, testable, and well-documented.

## üìÅ Module Overview

### **Core Pipeline Components**

**1. Backbone** (`backbone.py`) ‚úÖ
- **Input**: 4 camera views (384√ó384 RGB)
- **Output**: 64-channel features (12√ó12) per view
- **Options**: ResNet50 (default), ResNet18, EfficientNet-B0
- **Purpose**: Extract 2D features from each camera
- **Parameters**: 24.7M total, 1.2M trainable

**2. View Transform** (`view_transform.py`) ‚úÖ
- **Input**: 64-channel features (12√ó12) √ó 4 views
- **Output**: 64-channel BEV features (256√ó256)
- **Method**: Adaptive pooling + fusion (placeholder - not true LSS)
- **Purpose**: Project multi-view features to BEV space
- **Note**: This is a simplified placeholder, not the actual Lift-Splat-Shoot method

**3. BEV Encoder** (`bev_encoder.py`) ‚úÖ
- **Input**: 64-channel BEV features (256√ó256)
- **Output**: 128-channel encoded features (64√ó64)
- **Purpose**: Process BEV features with CNN layers
- **Architecture**: 64 ‚Üí 128 ‚Üí 128 channels

**4. Decoder** (`decoder.py`) ‚úÖ
- **Input**: 128-channel encoded features (64√ó64)
- **Output**: 7-class segmentation (256√ó256)
- **Purpose**: Generate final segmentation mask
- **Upsampling**: 64√ó64 ‚Üí 128√ó128 ‚Üí 256√ó256

**5. Model** (`model.py`) ‚úÖ
- **Purpose**: Combine all components into complete pipeline
- **Input**: 4 camera views ‚Üí **Output**: 7-class BEV segmentation
- **Integration**: End-to-end forward pass

### **Data & Training Components**

**6. Dataset** (`dataset.py`) ‚úÖ
- **Classes**: 7 classes for autonomous driving
- **Mapping**: 0:unlabeled, 1:car, 2:vegetation, 3:road, 4:terrain, 5:guard_rail, 6:sidewalk
- **Purpose**: Load multi-view camera images and BEV labels
- **Samples**: 33,199 train + 3,731 validation

**7. Training** (`training.py`) ‚úÖ
- **Purpose**: End-to-end training with Focal Loss for class imbalance
- **Features**: Mixed precision, gradient clipping, comprehensive metrics
- **Loss**: Focal Loss (Œ±=1.0, Œ≥=2.0) for class imbalance

### **Utility & Management Components**

**8. Visualization** (`visualization.py`) ‚úÖ
- **Purpose**: 4 camera views + BEV comparison visualization
- **Features**: Ground truth vs prediction, class distribution
- **Output**: Comprehensive sample analysis

**9. Runs Management** (`runs_manager.py`) ‚úÖ
- **Purpose**: Experiment organization and checkpoint management
- **Features**: Automatic run directories, metrics tracking, CSV export
- **Structure**: `runs/run_TIMESTAMP/` with checkpoints and visualizations

**10. Weights & Biases** (`wandb_logger.py`) ‚úÖ
- **Purpose**: Experiment tracking and visualization
- **Features**: Metrics logging, model info, image logging
- **Integration**: Optional wandb support

**11. Temporal Fusion** (`temporal_fusion.py`) ‚úÖ
- **Purpose**: Fuse features from multiple timesteps
- **Input**: BEV features from multiple timesteps
- **Output**: Fused BEV features for better stability
- **Methods**: Attention, Conv3D, LSTM fusion
- **Benefits**: Better occlusion handling, temporal consistency

### **Utility Scripts**

**12. Class Weight Calculation** (`calculate_class_weights.py`) ‚úÖ
- **Purpose**: Pre-calculate class weights for imbalanced dataset
- **Method**: Inverse frequency weighting with sqrt normalization
- **Output**: Updates config file with calculated weights
- **Usage**: `python -m src.calculate_class_weights --config configs/config.yaml --num-samples 20`

**13. Model Evaluation** (`evaluate_model.py`) ‚úÖ
- **Purpose**: Evaluate trained models on validation samples
- **Features**: Random sampling, metrics calculation, visualization generation
- **Output**: Evaluation results and visualizations saved to run directory
- **Usage**: `python -m src.evaluate_model --checkpoint runs/run_TIMESTAMP/checkpoints/best_model.pth --num-samples 20`

## üîß Module Dependencies

```
model.py
‚îú‚îÄ‚îÄ backbone.py
‚îú‚îÄ‚îÄ view_transform.py
‚îú‚îÄ‚îÄ bev_encoder.py
‚îú‚îÄ‚îÄ decoder.py
‚îî‚îÄ‚îÄ temporal_fusion.py (optional)

training.py
‚îú‚îÄ‚îÄ model.py
‚îú‚îÄ‚îÄ dataset.py
‚îú‚îÄ‚îÄ runs_manager.py
‚îú‚îÄ‚îÄ wandb_logger.py
‚îî‚îÄ‚îÄ visualization.py

evaluate_model.py
‚îú‚îÄ‚îÄ model.py
‚îú‚îÄ‚îÄ dataset.py
‚îú‚îÄ‚îÄ training.py (for metrics)
‚îî‚îÄ‚îÄ visualization.py
```

## üß™ Testing Individual Modules

```bash
# Test core components
python -m src.backbone
python -m src.view_transform
python -m src.bev_encoder
python -m src.decoder
python -m src.temporal_fusion

# Test complete model
python -m src.model

# Test data loading
python -m src.dataset

# Test utilities
python -m src.calculate_class_weights --help
python -m src.evaluate_model --help
```

## üìä Expected Output Shapes

| Module | Input Shape | Output Shape | Purpose |
|--------|-------------|--------------|---------|
| **Backbone** | `[B, 3, 384, 384]` √ó 4 | `[B, 64, 12, 12]` √ó 4 | Feature extraction |
| **View Transform** | `[B, 64, 12, 12]` √ó 4 | `[B, 64, 256, 256]` | BEV projection |
| **BEV Encoder** | `[B, 64, 256, 256]` | `[B, 128, 64, 64]` | Feature processing |
| **Decoder** | `[B, 128, 64, 64]` | `[B, 7, 256, 256]` | Segmentation |
| **Temporal Fusion** | `[B, 64, 256, 256]` √ó T | `[B, 64, 256, 256]` | Multi-timestep fusion |

Where:
- `B` = Batch size
- `T` = Number of timesteps (3 for temporal fusion)
- `384√ó384` = Input image resolution
- `256√ó256` = BEV output resolution
- `7` = Number of classes


## ‚ö†Ô∏è Implementation Notes

### **View Transform Limitation**
The current `view_transform.py` implementation is **NOT** the actual Lift-Splat-Shoot (LSS) method. It uses:

**Current Implementation (Placeholder):**
- Adaptive pooling to resize features from 12√ó12 to 256√ó256
- Simple concatenation of multi-view features
- Basic convolution for feature fusion

**True LSS Implementation Would Include:**
- **Lift**: Predict depth distributions for each pixel
- **Splat**: Project features to 3D space using camera geometry
- **Shoot**: Rasterize to BEV grid with proper depth handling

### **Future Enhancement**
A proper LSS implementation would significantly improve:
- **Depth awareness** in BEV projection
- **Geometric accuracy** of multi-view fusion
- **Occlusion handling** through proper 3D reasoning

---

For the complete pipeline usage, see the main [README.md](../readme.md) file.
